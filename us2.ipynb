{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dresser.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMHY4hkmmEEWzVq+C3MpCWz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hackerinheels/dresser/blob/main/us2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9tCUl1Dvitl"
      },
      "source": [
        "This is a notebook that builds a model to identify a traditional dresser against a contemporary or industrial style dresser."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEYbpdGSvuFx"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import PIL.Image\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChcNpaA9wTSB",
        "outputId": "1b466e50-11f0-4966-d1ff-e9047645d6e0"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MKl1Zn30xHJ"
      },
      "source": [
        "import pathlib\n",
        "trainingdata_url = \"https://storage.googleapis.com/furndata/us.zip\"\n",
        "testdata_url = \"https://storage.googleapis.com/furndata/test.zip\"\n",
        "data_dir = tf.keras.utils.get_file(origin=trainingdata_url, \n",
        "                                   fname='us', \n",
        "                                   untar=True)\n",
        "data_dir = pathlib.Path(data_dir)\n",
        "test_data_dir = tf.keras.utils.get_file(origin=testdata_url, \n",
        "                                   fname='test', \n",
        "                                   untar=True)\n",
        "test_data_dir = pathlib.Path(test_data_dir)\n",
        "!unzip /root/.keras/datasets/us.tar.gz -d ./\n",
        "!unzip /root/.keras/datasets/test.tar.gz -d ./"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltCZq6i6slFy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd79ab42-f402-49c4-ecf0-658f1f112299"
      },
      "source": [
        "training_data_dir = \"/content/us/\"\n",
        "training_data_dir = pathlib.Path(training_data_dir)\n",
        "test_data_dir = \"/content/test/\"\n",
        "test_data_dir = pathlib.Path(test_data_dir)\n",
        "# First run unzip data.tar.gz\n",
        "print(len(list(training_data_dir.glob('*/*'))))\n",
        "print(len(list(test_data_dir.glob('*/*'))))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "122\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6E-qp_BG2cyk"
      },
      "source": [
        "batch_size = 32\n",
        "img_height = 256\n",
        "img_width = 256"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYfBCRS72hZt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e71431a5-4c3b-4b7b-81fb-0eb099dc4da8"
      },
      "source": [
        "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  training_data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"training\",\n",
        "  labels='inferred',\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 119 files belonging to 2 classes.\n",
            "Using 96 files for training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r33VmY-HqIQP",
        "outputId": "8bcbd361-0118-4fa0-ddd4-67eb5399b644"
      },
      "source": [
        "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  training_data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"validation\",\n",
        "  labels='inferred',\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 119 files belonging to 2 classes.\n",
            "Using 23 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxmaQsmNUA7p",
        "outputId": "f5d189ca-7747-4e42-cda7-a310d0bbbee6"
      },
      "source": [
        "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  test_data_dir,\n",
        "  labels='inferred',\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iz5OoGjqO6Y",
        "outputId": "9822741a-2500-4ae4-e0b3-6488afb97dc8"
      },
      "source": [
        "class_names = train_ds.class_names\n",
        "print(class_names)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['aeshaan', 'me']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASQBs5TYqSnm"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_ds.take(1):\n",
        "  for i in range(9):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    plt.title(class_names[labels[i]])\n",
        "    plt.axis(\"off\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vs78Yut5qszI"
      },
      "source": [
        "num_classes = 2\n",
        "IMG_SIZE = 256\n",
        "model = tf.keras.Sequential([\n",
        "   \n",
        "  tf.keras.layers.experimental.preprocessing.Resizing(IMG_SIZE, IMG_SIZE),\n",
        "  tf.keras.layers.experimental.preprocessing.Rescaling(1./255),\n",
        "  tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
        "  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
        "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.MaxPool2D(),\n",
        "  tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.MaxPool2D(),\n",
        "  tf.keras.layers.Dropout(0.1),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(1)\n",
        "])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SLpunqKqxmm"
      },
      "source": [
        "model.compile(\n",
        "  optimizer='adam',\n",
        "  loss=tf.losses.BinaryCrossentropy(from_logits=True),\n",
        "  metrics=['accuracy'])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-W-pxCHqz_9",
        "outputId": "e9eea306-e484-4aed-f9d7-4dcaf5deb683"
      },
      "source": [
        "model.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  epochs=10\n",
        ")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "3/3 [==============================] - 20s 5s/step - loss: 24.5916 - accuracy: 0.5833 - val_loss: 3.0822 - val_accuracy: 0.7391\n",
            "Epoch 2/10\n",
            "3/3 [==============================] - 18s 5s/step - loss: 24.1884 - accuracy: 0.6667 - val_loss: 3.6055 - val_accuracy: 0.7391\n",
            "Epoch 3/10\n",
            "3/3 [==============================] - 18s 5s/step - loss: 9.4289 - accuracy: 0.8021 - val_loss: 2.1684 - val_accuracy: 0.7391\n",
            "Epoch 4/10\n",
            "3/3 [==============================] - 18s 5s/step - loss: 10.7698 - accuracy: 0.7812 - val_loss: 1.0535 - val_accuracy: 0.7391\n",
            "Epoch 5/10\n",
            "3/3 [==============================] - 18s 5s/step - loss: 10.2042 - accuracy: 0.7604 - val_loss: 1.2333 - val_accuracy: 0.7826\n",
            "Epoch 6/10\n",
            "3/3 [==============================] - 18s 5s/step - loss: 5.1343 - accuracy: 0.8333 - val_loss: 1.4871 - val_accuracy: 0.7391\n",
            "Epoch 7/10\n",
            "3/3 [==============================] - 18s 5s/step - loss: 4.2159 - accuracy: 0.8438 - val_loss: 1.9568 - val_accuracy: 0.8261\n",
            "Epoch 8/10\n",
            "3/3 [==============================] - 18s 5s/step - loss: 3.5752 - accuracy: 0.8854 - val_loss: 2.0029 - val_accuracy: 0.7826\n",
            "Epoch 9/10\n",
            "3/3 [==============================] - 18s 5s/step - loss: 4.0232 - accuracy: 0.8958 - val_loss: 3.1068 - val_accuracy: 0.6087\n",
            "Epoch 10/10\n",
            "3/3 [==============================] - 18s 5s/step - loss: 3.6646 - accuracy: 0.8958 - val_loss: 6.3741 - val_accuracy: 0.6087\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2f30dec2d0>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nH84gHH6Wmog",
        "outputId": "3bf93a1f-8e42-4929-c118-9e1be8e39c48"
      },
      "source": [
        "# Evaluate the model on the test data using `evaluate`\n",
        "print(\"Evaluate on test data\")\n",
        "results = model.evaluate(test_ds)\n",
        "print(\"test loss, test acc:\", results)\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluate on test data\n",
            "1/1 [==============================] - 1s 887ms/step - loss: 0.6343 - accuracy: 0.8889\n",
            "test loss, test acc: [0.6343067288398743, 0.8888888955116272]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0gwhSplXl0n",
        "outputId": "d64c7f1f-87cc-4a19-9468-da44b888bb8e"
      },
      "source": [
        "# Generate predictions (probabilities -- the output of the last layer)\n",
        "# on new data using `predict`\n",
        "print(\"Generate predictions for 3 samples\")\n",
        "predictions = model.predict(test_ds)\n",
        "print(\"predictions shape:\", predictions.shape)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generate predictions for 3 samples\n",
            "predictions shape: (9, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRc0TUTNynJX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8df4505e-5a3b-49cc-d41d-ffd0d5ee0f09"
      },
      "source": [
        "checkpoint_path = \"/content/cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)\n",
        "\n",
        "# Train the model with the new callback\n",
        "model.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  epochs=1,\n",
        "  callbacks=[cp_callback]\n",
        ")\n",
        "# This may generate warnings related to saving the state of the optimizer.\n",
        "# These warnings (and similar warnings throughout this notebook)\n",
        "# are in place to discourage outdated usage, and can be ignored."
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3/3 [==============================] - 18s 5s/step - loss: 2.1922 - accuracy: 0.9062 - val_loss: 20.4346 - val_accuracy: 0.2609\n",
            "\n",
            "Epoch 00001: saving model to /content/cp.ckpt\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2f30a78350>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SjInbA-zpVf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd7f80fd-32a5-4d5b-e31e-738ae19adb1f"
      },
      "source": [
        "# Loads the weights\n",
        "model.load_weights(checkpoint_path)\n",
        "\n",
        "# Re-evaluate the model\n",
        "#loss, acc = model.evaluate(test_images, test_labels, verbose=2)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f33323891d0>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGQykYbdKv-I"
      },
      "source": [
        "NOW Trying hyperparameterization automation using Keras Tuner with a diff model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dx5FNG7maO3z"
      },
      "source": [
        "!pip install -q -U keras-tuner"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28M1i7hELdnH"
      },
      "source": [
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from tensorflow.keras.layers import BatchNormalization, experimental\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras_tuner.tuners import RandomSearch\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clB-2yxIK5Ay"
      },
      "source": [
        "# define cnn model\n",
        "def build_model_1(hp):\n",
        "\tmodel = Sequential()\n",
        "\t#model.add(experimental.preprocessing.Rescaling(1./255))\n",
        "\t#model.add(experimental.preprocessing.Resizing(256, 256))\n",
        "\thp_filters = hp.Int('filters', min_value = 32, max_value = 64, step = 32)\n",
        "\tmodel.add(Conv2D(filters=hp_filters, kernel_size=(3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(256,256, 3)))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(MaxPooling2D((2, 2)))\n",
        "\tmodel.add(Dropout(0.1))\n",
        "\tmodel.add(Flatten())\n",
        "\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
        "\tmodel.add(Dense(1, activation='softmax'))\n",
        "\t# compile model\n",
        "\t#opt = SGD(lr=0.001, momentum=0.9)\n",
        "\thp_learning_rate = hp.Choice('learning_rate', values = [1e-2, 1e-3])\n",
        "\topt = Adam(learning_rate=hp_learning_rate)#, momentum=0.9)\n",
        "\tmodel.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\tmodel.summary()\n",
        "\treturn model"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2jVyadULVhg",
        "outputId": "6ec9a858-2365-4f1b-f96e-884032a65b35"
      },
      "source": [
        "\n",
        "tuner = RandomSearch(\n",
        "    build_model_1,\n",
        "    objective='val_accuracy',\n",
        "    max_trials=4,\n",
        "    executions_per_trial=1)\n",
        "#,\n",
        "#    directory='my_dir1') #change the directory name here  when rerunning the cell else it gives \"Oracle exit error\" \n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 256, 256, 32)      896       \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 256, 256, 32)      128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 128, 128, 32)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 128, 128, 32)      9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 128, 128, 32)      128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 64, 64, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64, 64, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 131072)            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               16777344  \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 16,787,873\n",
            "Trainable params: 16,787,745\n",
            "Non-trainable params: 128\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wu053UGqhFg6"
      },
      "source": [
        "!\\rm -rf ./untitled_project/oracle.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Rq-5iShSlqN"
      },
      "source": [
        "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vKDFA4qrLYdd",
        "outputId": "bad2d189-ca8f-4511-b23e-39a537c5b67d"
      },
      "source": [
        "tuner.search(train_ds, validation_data=val_ds,\n",
        "  epochs=3)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 4 Complete [00h 02m 03s]\n",
            "val_accuracy: 0.739130437374115\n",
            "\n",
            "Best val_accuracy So Far: 0.739130437374115\n",
            "Total elapsed time: 00h 05m 52s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxZqa9pLtxvS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6c9dbb4-d32c-4066-8dfd-6d16d8f78c48"
      },
      "source": [
        "tuner.results_summary()\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results summary\n",
            "Results in ./untitled_project\n",
            "Showing 10 best trials\n",
            "Objective(name='val_accuracy', direction='max')\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "filters: 64\n",
            "learning_rate: 0.001\n",
            "Score: 0.739130437374115\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "filters: 32\n",
            "learning_rate: 0.001\n",
            "Score: 0.739130437374115\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "filters: 32\n",
            "learning_rate: 0.01\n",
            "Score: 0.739130437374115\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "filters: 64\n",
            "learning_rate: 0.01\n",
            "Score: 0.739130437374115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaUWzjMGdRPm"
      },
      "source": [
        "A few things remaining\n",
        "1. Use tuner results\n",
        "2. Add TF displayboard\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HayukWJdW8L"
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "# Load the TensorBoard notebook extension.\n",
        "%load_ext tensorboard"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEc4suPLdfe9",
        "outputId": "fdc0e1de-8e52-4784-8faf-0918ad9ba28d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for x, y in train_ds.take(1):\n",
        "  print(\"Shape: \", x.shape)\n",
        "  print(\"Label: \", y[0], \"->\", class_names[y[0]])"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape:  (32, 256, 256, 3)\n",
            "Label:  tf.Tensor(1, shape=(), dtype=int32) -> me\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKGMwM0HefhH"
      },
      "source": [
        "#Visualizing a single image\n",
        "from datetime import datetime\n",
        "import io\n",
        "import itertools\n",
        "from packaging import version\n",
        "\n",
        "# Clear out any prior log data.\n",
        "!rm -rf logs\n",
        "\n",
        "# Sets up a timestamped log directory.\n",
        "logdir = \"logs/train_data/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "# Creates a file writer for the log directory.\n",
        "file_writer = tf.summary.create_file_writer(logdir)\n",
        "\n",
        "img = x[0]\n",
        "# Using the file writer, log the reshaped image.\n",
        "with file_writer.as_default():\n",
        "  tf.summary.image(\"Training data\", x, step=0)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6PDdZ48fdEy"
      },
      "source": [
        "# Reshape the image for the Summary API.\n",
        "img = np.reshape(x[0], (-1, 256, 256, 3))\n",
        "with file_writer.as_default():\n",
        "  tf.summary.image(\"Training data\", img, step=0)"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gM04LoVtg7or",
        "outputId": "b8a23e2a-520e-438f-dc49-1d6ddec6178c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install tensorboard --ignore-installed\n",
        "\n"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboard\n",
            "  Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 5.0 MB/s \n",
            "\u001b[?25hCollecting protobuf>=3.6.0\n",
            "  Downloading protobuf-3.18.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 50.0 MB/s \n",
            "\u001b[?25hCollecting grpcio>=1.24.3\n",
            "  Downloading grpcio-1.41.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.9 MB 42.4 MB/s \n",
            "\u001b[?25hCollecting setuptools>=41.0.0\n",
            "  Downloading setuptools-58.1.0-py3-none-any.whl (816 kB)\n",
            "\u001b[K     |████████████████████████████████| 816 kB 39.9 MB/s \n",
            "\u001b[?25hCollecting absl-py>=0.4\n",
            "  Downloading absl_py-0.14.0-py3-none-any.whl (131 kB)\n",
            "\u001b[K     |████████████████████████████████| 131 kB 73.6 MB/s \n",
            "\u001b[?25hCollecting requests<3,>=2.21.0\n",
            "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 757 kB/s \n",
            "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
            "  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\n",
            "\u001b[K     |████████████████████████████████| 781 kB 71.7 MB/s \n",
            "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 43.8 MB/s \n",
            "\u001b[?25hCollecting numpy>=1.12.0\n",
            "  Downloading numpy-1.21.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 184 kB/s \n",
            "\u001b[?25hCollecting markdown>=2.6.8\n",
            "  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting google-auth<2,>=1.6.3\n",
            "  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n",
            "\u001b[K     |████████████████████████████████| 152 kB 57.1 MB/s \n",
            "\u001b[?25hCollecting werkzeug>=0.11.15\n",
            "  Downloading Werkzeug-2.0.1-py3-none-any.whl (288 kB)\n",
            "\u001b[K     |████████████████████████████████| 288 kB 53.3 MB/s \n",
            "\u001b[?25hCollecting wheel>=0.26\n",
            "  Downloading wheel-0.37.0-py2.py3-none-any.whl (35 kB)\n",
            "Collecting six\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting cachetools<5.0,>=2.0.0\n",
            "  Downloading cachetools-4.2.3-py3-none-any.whl (8.6 kB)\n",
            "Collecting pyasn1-modules>=0.2.1\n",
            "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
            "\u001b[K     |████████████████████████████████| 155 kB 38.0 MB/s \n",
            "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
            "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
            "Collecting requests-oauthlib>=0.7.0\n",
            "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
            "Collecting importlib-metadata\n",
            "  Downloading importlib_metadata-4.8.1-py3-none-any.whl (17 kB)\n",
            "Collecting pyasn1<0.5.0,>=0.4.6\n",
            "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 4.9 MB/s \n",
            "\u001b[?25hCollecting charset-normalizer~=2.0.0\n",
            "  Downloading charset_normalizer-2.0.6-py3-none-any.whl (37 kB)\n",
            "Collecting certifi>=2017.4.17\n",
            "  Downloading certifi-2021.5.30-py2.py3-none-any.whl (145 kB)\n",
            "\u001b[K     |████████████████████████████████| 145 kB 49.9 MB/s \n",
            "\u001b[?25hCollecting idna<4,>=2.5\n",
            "  Downloading idna-3.2-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 6.0 MB/s \n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 56.5 MB/s \n",
            "\u001b[?25hCollecting oauthlib>=3.0.0\n",
            "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
            "\u001b[K     |████████████████████████████████| 146 kB 53.0 MB/s \n",
            "\u001b[?25hCollecting zipp>=0.5\n",
            "  Downloading zipp-3.6.0-py3-none-any.whl (5.3 kB)\n",
            "Collecting typing-extensions>=3.6.4\n",
            "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: urllib3, pyasn1, idna, charset-normalizer, certifi, zipp, typing-extensions, six, setuptools, rsa, requests, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, importlib-metadata, google-auth, wheel, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, protobuf, numpy, markdown, grpcio, google-auth-oauthlib, absl-py, tensorboard\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.6.0 requires numpy~=1.19.2, but you have numpy 1.21.2 which is incompatible.\n",
            "tensorflow 2.6.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
            "tensorflow 2.6.0 requires typing-extensions~=3.7.4, but you have typing-extensions 3.10.0.2 which is incompatible.\n",
            "tensorflow-metadata 1.2.0 requires absl-py<0.13,>=0.9, but you have absl-py 0.14.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
            "flask 1.1.4 requires Werkzeug<2.0,>=0.15, but you have werkzeug 2.0.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed absl-py-0.14.0 cachetools-4.2.3 certifi-2021.5.30 charset-normalizer-2.0.6 google-auth-1.35.0 google-auth-oauthlib-0.4.6 grpcio-1.41.0 idna-3.2 importlib-metadata-4.8.1 markdown-3.3.4 numpy-1.21.2 oauthlib-3.1.1 protobuf-3.18.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-2.26.0 requests-oauthlib-1.3.0 rsa-4.7.2 setuptools-58.1.0 six-1.16.0 tensorboard-2.6.0 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.0 typing-extensions-3.10.0.2 urllib3-1.26.7 werkzeug-2.0.1 wheel-0.37.0 zipp-3.6.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "absl",
                  "cachetools",
                  "certifi",
                  "google",
                  "grpc",
                  "idna",
                  "numpy",
                  "pkg_resources",
                  "pyasn1",
                  "pyasn1_modules",
                  "requests",
                  "rsa",
                  "six",
                  "tensorboard",
                  "typing_extensions",
                  "urllib3",
                  "zipp"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYDkMbkCemVn",
        "outputId": "5e5762fd-a2c2-4057-8c5b-50b2a06fd028",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "with file_writer.as_default():\n",
        "  # Don't forget to reshape.\n",
        "  tf.summary.image(\"32 training data examples\", x, max_outputs=25, step=0)\n",
        "\n",
        "%tensorboard --logdir logs/train_data -bind_all\n"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ERROR: Failed to launch TensorBoard (exited with 2).\n",
              "Contents of stderr:\n",
              "usage: tensorboard [-h] [--helpfull] [--logdir PATH] [--logdir_spec PATH_SPEC]\n",
              "                   [--host ADDR] [--bind_all] [--port PORT]\n",
              "                   [--reuse_port BOOL] [--load_fast {false,auto,true}]\n",
              "                   [--extra_data_server_flags EXTRA_DATA_SERVER_FLAGS]\n",
              "                   [--grpc_creds_type {local,ssl,ssl_dev}]\n",
              "                   [--grpc_data_provider PORT] [--purge_orphaned_data BOOL]\n",
              "                   [--db URI] [--db_import] [--inspect] [--version_tb]\n",
              "                   [--tag TAG] [--event_file PATH] [--path_prefix PATH]\n",
              "                   [--window_title TEXT] [--max_reload_threads COUNT]\n",
              "                   [--reload_interval SECONDS] [--reload_task TYPE]\n",
              "                   [--reload_multifile BOOL]\n",
              "                   [--reload_multifile_inactive_secs SECONDS]\n",
              "                   [--generic_data TYPE]\n",
              "                   [--samples_per_plugin SAMPLES_PER_PLUGIN]\n",
              "                   [--whatif-use-unsafe-custom-prediction YOUR_CUSTOM_PREDICT_FUNCTION.py]\n",
              "                   [--whatif-data-dir PATH]\n",
              "                   {serve,dev} ...\n",
              "tensorboard: error: unrecognized arguments: -bind_all"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}